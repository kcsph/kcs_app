{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\" Base structures and functions for the sume module.\n",
    "\n",
    "    Base contains the Sentence, LoadFile and State classes.\n",
    "\n",
    "\n",
    "    author: florian boudin (florian.boudin@univ-nantes.fr)\n",
    "    version: 0.1\n",
    "    date: Nov. 2014\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import os\n",
    "import codecs\n",
    "from collections import Counter\n",
    "\n",
    "class State:\n",
    "    \"\"\" State class\n",
    "\n",
    "    Internal class used as a structure to keep track of the search state in \n",
    "    the tabu_search method.\n",
    "\n",
    "    Args:\n",
    "        subset (set): a subset of sentences\n",
    "        concepts (Counter): a set of concepts for the subset\n",
    "        length (int): the length in words\n",
    "        score (int): the score for the subset\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.subset = set()\n",
    "        self.concepts = Counter()\n",
    "        self.length = 0\n",
    "        self.score = 0\n",
    "\n",
    "class Sentence:\n",
    "    \"\"\"The sentence data structure.\n",
    "\n",
    "    Args: \n",
    "        tokens (list of str): the list of word tokens.\n",
    "        doc_id (str): the identifier of the document from which the sentence\n",
    "          comes from.\n",
    "        position (int): the position of the sentence in the source document.\n",
    "    \"\"\"\n",
    "    def __init__(self, tokens, doc_id, position):\n",
    "\n",
    "        self.tokens = tokens\n",
    "        \"\"\" tokens as a list. \"\"\"\n",
    "\n",
    "        self.doc_id = doc_id\n",
    "        \"\"\" document identifier of the sentence. \"\"\"\n",
    "\n",
    "        self.position = position\n",
    "        \"\"\" position of the sentence within the document. \"\"\"\n",
    "\n",
    "        self.concepts = []\n",
    "        \"\"\" concepts of the sentence. \"\"\"\n",
    "\n",
    "        self.untokenized_form = ''\n",
    "        \"\"\" untokenized form of the sentence. \"\"\"\n",
    "\n",
    "        self.length = 0\n",
    "        \"\"\" length of the untokenized sentence. \"\"\"\n",
    "\n",
    "class LoadFile(object):\n",
    "    \"\"\"Objects which inherit from this class have read file functions.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_directory):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_file (str): the path of the input file.\n",
    "            use_stems (bool): whether stems should be used instead of words,\n",
    "              defaults to False.\n",
    "\n",
    "        \"\"\"\n",
    "        self.input_directory = input_directory\n",
    "        self.sentences = []\n",
    "\n",
    "    def read_documents(self, file_extension=\"txt\"):\n",
    "        \"\"\"Read the input files in the given directory.\n",
    "\n",
    "        Load the input files and populate the sentence list. Input files are\n",
    "        expected to be in one tokenized sentence per line format.\n",
    "\n",
    "        Args:\n",
    "            file_extension (str): the file extension for input documents,\n",
    "              defaults to txt.\n",
    "        \"\"\"\n",
    "        for infile in os.listdir(self.input_directory):\n",
    "\n",
    "            # skip files with wrong extension\n",
    "            if not infile.endswith(file_extension):\n",
    "                continue\n",
    "\n",
    "            with codecs.open(self.input_directory + '/' + infile,\n",
    "                             'r',\n",
    "                             'utf-8') as f:\n",
    "\n",
    "                # load the sentences\n",
    "                lines = f.readlines()\n",
    "\n",
    "                # loop over sentences\n",
    "                for i in range(len(lines)):\n",
    "\n",
    "                    # split the sentence into tokens\n",
    "                    tokens = lines[i].strip().split(' ')\n",
    "\n",
    "                    # add the sentence\n",
    "                    if len(tokens) > 0:\n",
    "                        sentence = Sentence(tokens, infile, i)\n",
    "                        untokenized_form = untokenize(tokens)\n",
    "                        sentence.untokenized_form = untokenized_form\n",
    "                        sentence.length = len(untokenized_form.split(' '))\n",
    "                        self.sentences.append(sentence)\n",
    "\n",
    "def untokenize(tokens):\n",
    "    \"\"\"Untokenizing a list of tokens. \n",
    "\n",
    "    Args:\n",
    "        tokens (list of str): the list of tokens to untokenize.\n",
    "\n",
    "    Returns:\n",
    "        a string\n",
    "\n",
    "    \"\"\"\n",
    "    text = u' '.join(tokens)\n",
    "    text = re.sub(u\"\\s+\", u\" \", text.strip())\n",
    "    text = re.sub(u\" ('[a-z]) \", u\"\\g<1> \", text)\n",
    "    text = re.sub(u\" ([\\.;,-]) \", u\"\\g<1> \", text)\n",
    "    text = re.sub(u\" ([\\.;,-?!])$\", u\"\\g<1>\", text)\n",
    "    text = re.sub(u\" _ (.+) _ \", u\" _\\g<1>_ \", text)\n",
    "    text = re.sub(u\" \\$ ([\\d\\.]+) \", u\" $\\g<1> \", text)\n",
    "    text = text.replace(u\" ' \", u\"' \")\n",
    "    text = re.sub(u\"([\\W\\s])\\( \", u\"\\g<1>(\", text)\n",
    "    text = re.sub(u\" \\)([\\W\\s])\", u\")\\g<1>\", text)\n",
    "    text = text.replace(u\"`` \", u\"``\")\n",
    "    text = text.replace(u\" ''\", u\"''\")\n",
    "    text = text.replace(u\" n't\", u\"n't\")\n",
    "    text = re.sub(u'(^| )\" ([^\"]+) \"( |$)', u'\\g<1>\"\\g<2>\"\\g<3>', text)\n",
    "\n",
    "    # times\n",
    "    text = re.sub('(\\d+) : (\\d+ [ap]\\.m\\.)', '\\g<1>:\\g<2>', text)\n",
    "\n",
    "    text = re.sub('^\" ', '\"', text)\n",
    "    text = re.sub(' \"$', '\"', text)\n",
    "    text = re.sub(u\"\\s+\", u\" \", text.strip())\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\" Concept-based ILP summarization methods.\n",
    "\n",
    "    authors: Florian Boudin (florian.boudin@univ-nantes.fr)\n",
    "             Hugo Mougard (hugo.mougard@univ-nantes.fr)\n",
    "    version: 0.2\n",
    "    date: May 2015\n",
    "\"\"\"\n",
    "\n",
    "# from sume.base import Sentence, State, untokenize, LoadFile\n",
    "\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "import os\n",
    "import re\n",
    "import codecs\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import nltk\n",
    "import pulp\n",
    "\n",
    "class ConceptBasedILPSummarizer(LoadFile):\n",
    "    \"\"\"Implementation of the concept-based ILP model for summarization.\n",
    "\n",
    "    The original algorithm was published and described in:\n",
    "\n",
    "      * Dan Gillick and Benoit Favre, A Scalable Global Model for Summarization,\n",
    "        *Proceedings of the NAACL HLT Workshop on Integer Linear Programming for\n",
    "        Natural Language Processing*, pages 10â€“18, 2009.\n",
    "        \n",
    "    \"\"\"\n",
    "    def __init__(self, input_directory):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_directory (str): the directory from which text documents to\n",
    "              be summarized are loaded.\n",
    "\n",
    "        \"\"\"\n",
    "        self.input_directory = input_directory\n",
    "        self.sentences = []\n",
    "        self.weights = {}\n",
    "        self.c2s = defaultdict(set)\n",
    "        self.concept_sets = defaultdict(frozenset)\n",
    "        self.stoplist = nltk.corpus.stopwords.words('english')\n",
    "        self.stemmer = nltk.stem.snowball.SnowballStemmer('english')\n",
    "        self.word_frequencies = defaultdict(int)\n",
    "        self.w2s = defaultdict(set)\n",
    "\n",
    "    def extract_ngrams(self, n=2):\n",
    "        \"\"\"Extract the ngrams of words from the input sentences.\n",
    "\n",
    "        Args:\n",
    "            n (int): the number of words for ngrams, defaults to 2\n",
    "        \"\"\"\n",
    "        for i, sentence in enumerate(self.sentences):\n",
    "\n",
    "            # for each ngram of words\n",
    "            for j in range(len(sentence.tokens)-(n-1)):\n",
    "\n",
    "                # initialize ngram container\n",
    "                ngram = []\n",
    "\n",
    "                # for each token of the ngram\n",
    "                for k in range(j, j+n):\n",
    "                    ngram.append(sentence.tokens[k].lower())\n",
    "\n",
    "                # do not consider ngrams containing punctuation marks\n",
    "                marks = [t for t in ngram if not re.search('[a-zA-Z0-9]', t)]\n",
    "                if len(marks) > 0:\n",
    "                    continue\n",
    "\n",
    "                # do not consider ngrams composed of only stopwords\n",
    "                stops = [t for t in ngram if t in self.stoplist]\n",
    "                if len(stops) == len(ngram):\n",
    "                    continue\n",
    "\n",
    "                # stem the ngram\n",
    "                ngram = [self.stemmer.stem(t) for t in ngram]\n",
    "\n",
    "                # add the ngram to the concepts\n",
    "                self.sentences[i].concepts.append(' '.join(ngram))\n",
    "\n",
    "    def compute_document_frequency(self):\n",
    "        \"\"\"Compute the document frequency of each concept.\n",
    "\n",
    "        \"\"\"\n",
    "        for i in range(len(self.sentences)):\n",
    "\n",
    "            # for each concept\n",
    "            for concept in self.sentences[i].concepts:\n",
    "\n",
    "                # add the document id to the concept weight container\n",
    "                if concept not in self.weights:\n",
    "                    self.weights[concept] = set([])\n",
    "                self.weights[concept].add(self.sentences[i].doc_id)\n",
    "\n",
    "        # loop over the concepts and compute the document frequency\n",
    "        for concept in self.weights:\n",
    "            self.weights[concept] = len(self.weights[concept])\n",
    "\n",
    "    def compute_word_frequency(self):\n",
    "        \"\"\"Compute the frequency of each word in the set of documents. \"\"\"\n",
    "\n",
    "        for i, sentence in enumerate(self.sentences):\n",
    "            for token in sentence.tokens:\n",
    "                t = token.lower() \n",
    "                if not re.search('[a-zA-Z0-9]', t) or t in self.stoplist:\n",
    "                    continue\n",
    "                t = self.stemmer.stem(t)\n",
    "                self.w2s[t].add(i)\n",
    "                self.word_frequencies[t] += 1\n",
    "\n",
    "    def prune_sentences(self,\n",
    "                        mininum_sentence_length=5,\n",
    "                        remove_citations=True,\n",
    "                        remove_redundancy=True):\n",
    "        \"\"\"Prune the sentences.\n",
    "\n",
    "        Remove the sentences that are shorter than a given length, redundant\n",
    "        sentences and citations from entering the summary.\n",
    "\n",
    "        Args:\n",
    "            mininum_sentence_length (int): the minimum number of words for a\n",
    "              sentence to enter the summary, defaults to 5\n",
    "            remove_citations (bool): indicates that citations are pruned,\n",
    "              defaults to True\n",
    "            remove_redundancy (bool): indicates that redundant sentences are\n",
    "              pruned, defaults to True\n",
    "\n",
    "        \"\"\"\n",
    "        pruned_sentences = []\n",
    "\n",
    "        # loop over the sentences\n",
    "        for sentence in self.sentences:\n",
    "\n",
    "            # prune short sentences\n",
    "            if sentence.length < mininum_sentence_length:\n",
    "                continue\n",
    "\n",
    "            # prune citations\n",
    "            first_token, last_token = sentence.tokens[0], sentence.tokens[-1]\n",
    "            if remove_citations and \\\n",
    "               (first_token == u\"``\" or first_token == u'\"') and \\\n",
    "               (last_token == u\"''\" or first_token == u'\"'):\n",
    "                continue\n",
    "\n",
    "            # prune ___ said citations\n",
    "            # if remove_citations and \\\n",
    "            #     (sentence.tokens[0]==u\"``\" or sentence.tokens[0]==u'\"') and \\\n",
    "            #     re.search('(?i)(''|\") \\w{,30} (said|reported|told)\\.$',\n",
    "            #               sentence.untokenized_form):\n",
    "            #     continue\n",
    "\n",
    "            # prune identical and almost identical sentences\n",
    "            if remove_redundancy:\n",
    "                is_redundant = False\n",
    "                for prev_sentence in pruned_sentences:\n",
    "                    if sentence.tokens == prev_sentence.tokens:\n",
    "                        is_redundant = True\n",
    "                        break\n",
    "\n",
    "                if is_redundant:\n",
    "                    continue\n",
    "\n",
    "            # otherwise add the sentence to the pruned sentence container\n",
    "            pruned_sentences.append(sentence)\n",
    "\n",
    "        self.sentences = pruned_sentences\n",
    "\n",
    "    def prune_concepts(self, method=\"threshold\", value=3):\n",
    "        \"\"\"Prune the concepts for efficient summarization.\n",
    "\n",
    "        Args:\n",
    "            method (str): the method for pruning concepts that can be whether\n",
    "              by using a minimal value for concept scores (threshold) or using\n",
    "              the top-N highest scoring concepts (top-n), defaults to\n",
    "              threshold.\n",
    "            value (int): the value used for pruning concepts, defaults to 3.\n",
    "\n",
    "        \"\"\"\n",
    "        # 'threshold' pruning method\n",
    "        if method == \"threshold\":\n",
    "\n",
    "            # iterates over the concept weights\n",
    "            concepts = self.weights.keys()\n",
    "            for concept in concepts:\n",
    "                if self.weights[concept] < value:\n",
    "                    del self.weights[concept]\n",
    "\n",
    "        # 'top-n' pruning method\n",
    "        elif method == \"top-n\":\n",
    "\n",
    "            # sort concepts by scores\n",
    "            sorted_concepts = sorted(self.weights,\n",
    "                                     key=lambda x: self.weights[x],\n",
    "                                     reverse=True)\n",
    "\n",
    "            # iterates over the concept weights\n",
    "            concepts = self.weights.keys()\n",
    "            for concept in concepts:\n",
    "                if concept not in sorted_concepts[:value]:\n",
    "                    del self.weights[concept]\n",
    "\n",
    "        # iterates over the sentences\n",
    "        for i in range(len(self.sentences)):\n",
    "\n",
    "            # current sentence concepts\n",
    "            concepts = self.sentences[i].concepts\n",
    "\n",
    "            # prune concepts\n",
    "            self.sentences[i].concepts = [c for c in concepts\n",
    "                                          if c in self.weights]\n",
    "\n",
    "    def compute_c2s(self):\n",
    "        \"\"\"Compute the inverted concept to sentences dictionary. \"\"\"\n",
    "\n",
    "        for i, sentence in enumerate(self.sentences):\n",
    "            for concept in sentence.concepts:\n",
    "                self.c2s[concept].add(i)\n",
    "\n",
    "    def compute_concept_sets(self):\n",
    "        \"\"\"Compute the concept sets for each sentence.\"\"\"\n",
    "\n",
    "        for i, sentence in enumerate(self.sentences):\n",
    "            for concept in sentence.concepts:\n",
    "                self.concept_sets[i] |= {concept}\n",
    "\n",
    "    def greedy_approximation(self, summary_size=100):\n",
    "        \"\"\"Greedy approximation of the ILP model.\n",
    "\n",
    "        Args:\n",
    "            summary_size (int): the maximum size in words of the summary,\n",
    "              defaults to 100.\n",
    "\n",
    "        Returns:\n",
    "            (value, set) tuple (int, list): the value of the approximated\n",
    "              objective function and the set of selected sentences as a tuple.\n",
    "\n",
    "        \"\"\"\n",
    "        # initialize the inverted c2s dictionary if not already created\n",
    "        if not self.c2s:\n",
    "            self.compute_c2s()\n",
    "\n",
    "        # initialize weights\n",
    "        weights = {}\n",
    "\n",
    "        # initialize the score of the best singleton\n",
    "        best_singleton_score = 0\n",
    "\n",
    "        # compute indices of our sentences\n",
    "        sentences = range(len(self.sentences))\n",
    "\n",
    "        # compute initial weights and fill the reverse index\n",
    "        # while keeping track of the best singleton solution\n",
    "        for i, sentence in enumerate(self.sentences):\n",
    "            weights[i] = sum(self.weights[c] for c in set(sentence.concepts))\n",
    "            if sentence.length <= summary_size\\\n",
    "               and weights[i] > best_singleton_score:\n",
    "                best_singleton_score = weights[i]\n",
    "                best_singleton = i\n",
    "\n",
    "        # initialize the selected solution properties\n",
    "        sel_subset, sel_concepts, sel_length, sel_score = set(), set(), 0, 0\n",
    "\n",
    "        # greedily select a sentence\n",
    "        while True:\n",
    "\n",
    "            ###################################################################\n",
    "            # RETRIEVE THE BEST SENTENCE\n",
    "            ###################################################################\n",
    "\n",
    "            # sort the sentences by gain and reverse length\n",
    "            sort_sent = sorted(((weights[i] / float(self.sentences[i].length),\n",
    "                                 -self.sentences[i].length,\n",
    "                                 i)\n",
    "                                for i in sentences),\n",
    "                               reverse=True)\n",
    "\n",
    "            # select the first sentence that fits in the length limit\n",
    "            for sentence_gain, rev_length, sentence_index in sort_sent:\n",
    "                if sel_length - rev_length <= summary_size:\n",
    "                    break\n",
    "            # if we don't find a sentence, break out of the main while loop\n",
    "            else:\n",
    "                break\n",
    "\n",
    "            # if the gain is null, break out of the main while loop\n",
    "            if not weights[sentence_index]:\n",
    "                break\n",
    "\n",
    "            # update the selected subset properties\n",
    "            sel_subset.add(sentence_index)\n",
    "            sel_score += weights[sentence_index]\n",
    "            sel_length -= rev_length\n",
    "\n",
    "            # update sentence weights with the reverse index\n",
    "            for concept in set(self.sentences[sentence_index].concepts):\n",
    "                if concept not in sel_concepts:\n",
    "                    for sentence in self.c2s[concept]:\n",
    "                        weights[sentence] -= self.weights[concept]\n",
    "\n",
    "            # update the last selected subset property\n",
    "            sel_concepts.update(self.sentences[sentence_index].concepts)\n",
    "\n",
    "        # check if a singleton has a better score than our greedy solution\n",
    "        if best_singleton_score > sel_score:\n",
    "            return best_singleton_score, set([best_singleton])\n",
    "\n",
    "        # returns the (objective function value, solution) tuple\n",
    "        return sel_score, sel_subset\n",
    "\n",
    "    def tabu_search(self,\n",
    "                    summary_size=100,\n",
    "                    memory_size=10,\n",
    "                    iterations=100,\n",
    "                    mutation_size=2,\n",
    "                    mutation_group=True):\n",
    "        \"\"\"Greedy approximation of the ILP model with a tabu search\n",
    "          meta-heuristic.\n",
    "\n",
    "        Args:\n",
    "            summary_size (int): the maximum size in words of the summary,\n",
    "              defaults to 100.\n",
    "            memory_size (int): the maximum size of the pool of sentences\n",
    "              to ban at a given time, defaults at 5.\n",
    "            iterations (int): the number of iterations to run, defaults at\n",
    "              30.\n",
    "            mutation_size (int): number of sentences to unselect and add to\n",
    "              the tabu list at each iteration.\n",
    "            mutation_group (boolean): flag to consider the mutations as a\n",
    "              group: we'll check sentence combinations in the tabu list, not\n",
    "              sentences alone.\n",
    "        Returns:\n",
    "            (value, set) tuple (int, list): the value of the approximated\n",
    "              objective function and the set of selected sentences as a tuple.\n",
    "\n",
    "        \"\"\"\n",
    "        # compute concept to sentences and concept sets for each sentence\n",
    "        if not self.c2s:\n",
    "            self.compute_c2s()\n",
    "        if not self.concept_sets:\n",
    "            self.compute_concept_sets()\n",
    "\n",
    "        # initialize weights\n",
    "        weights = {}\n",
    "\n",
    "        # initialize the score of the best singleton\n",
    "        best_singleton_score = 0\n",
    "\n",
    "        # compute initial weights and fill the reverse index\n",
    "        # while keeping track of the best singleton solution\n",
    "        for i, sentence in enumerate(self.sentences):\n",
    "            weights[i] = sum(self.weights[c] for c in set(sentence.concepts))\n",
    "            if sentence.length <= summary_size\\\n",
    "               and weights[i] > best_singleton_score:\n",
    "                best_singleton_score = weights[i]\n",
    "                best_singleton = i\n",
    "\n",
    "        best_subset, best_score = None, 0\n",
    "        state = State()\n",
    "        for i in xrange(iterations):\n",
    "            queue = deque([], memory_size)\n",
    "            # greedily select sentences\n",
    "            state = self.select_sentences(summary_size,\n",
    "                                          weights,\n",
    "                                          state,\n",
    "                                          queue,\n",
    "                                          mutation_group)\n",
    "            if state.score > best_score:\n",
    "                best_subset = state.subset.copy()\n",
    "                best_score = state.score\n",
    "            to_tabu = set(random.sample(state.subset, mutation_size))\n",
    "            state = self.unselect_sentences(weights, state, to_tabu)\n",
    "            queue.extend(to_tabu)\n",
    "\n",
    "        # check if a singleton has a better score than our greedy solution\n",
    "        if best_singleton_score > best_score:\n",
    "            return best_singleton_score, set([best_singleton])\n",
    "\n",
    "        # returns the (objective function value, solution) tuple\n",
    "        return best_score, best_subset\n",
    "\n",
    "    def select_sentences(self,\n",
    "                         summary_size,\n",
    "                         weights,\n",
    "                         state,\n",
    "                         tabu_set,\n",
    "                         mutation_group):\n",
    "        \"\"\"Greedy sentence selector.\n",
    "\n",
    "        Args:\n",
    "            summary_size (int): the maximum size in words of the summary,\n",
    "              defaults to 100.\n",
    "            weights (dictionary): the sentence weights dictionary. This\n",
    "              dictionnary is updated during this method call (in-place).\n",
    "            state (State): the state of the tabu search from which to start\n",
    "              selecting sentences.\n",
    "            tabu_set (iterable): set of sentences that are tabu: this\n",
    "              selector will not consider them.\n",
    "            mutation_group (boolean): flag to consider the mutations as a\n",
    "              group: we'll check sentence combinations in the tabu list, not\n",
    "              sentences alone.\n",
    "\n",
    "        Returns:\n",
    "            state (State): the new state of the search. Also note that\n",
    "              weights is modified in-place.\n",
    "\n",
    "        \"\"\"\n",
    "        # greedily select a sentence while respecting the tabu\n",
    "        while True:\n",
    "\n",
    "            ###################################################################\n",
    "            # RETRIEVE THE BEST SENTENCE\n",
    "            ###################################################################\n",
    "\n",
    "            # sort the sentences by gain and reverse length\n",
    "            sort_sent = sorted(((weights[i] / float(self.sentences[i].length),\n",
    "                                 -self.sentences[i].length,\n",
    "                                 i)\n",
    "                                for i in range(len(self.sentences))\n",
    "                                if self.sentences[i].length + state.length <=\n",
    "                                summary_size),\n",
    "                               reverse=True)\n",
    "\n",
    "            # select the first sentence that fits in the length limit\n",
    "            for sentence_gain, rev_length, sentence_index in sort_sent:\n",
    "                if mutation_group:\n",
    "                    subset = state.subset | {sentence_index}\n",
    "                    for tabu in tabu_set:\n",
    "                        if tabu <= subset:\n",
    "                            break\n",
    "                    else:\n",
    "                        break\n",
    "                else:\n",
    "                    if sentence_index not in tabu_set:\n",
    "                        break\n",
    "            # if we don't find a sentence, break out of the main while loop\n",
    "            else:\n",
    "                break\n",
    "\n",
    "            # if the gain is null, break out of the main while loop\n",
    "            if not weights[sentence_index]:\n",
    "                break\n",
    "\n",
    "            # update state\n",
    "            state.subset |= {sentence_index}\n",
    "            state.concepts.update(self.concept_sets[sentence_index])\n",
    "            state.length -= rev_length\n",
    "            state.score += weights[sentence_index]\n",
    "\n",
    "            # update sentence weights with the reverse index\n",
    "            for concept in set(self.concept_sets[sentence_index]):\n",
    "                if state.concepts[concept] == 1:\n",
    "                    for sentence in self.c2s[concept]:\n",
    "                        weights[sentence] -= self.weights[concept]\n",
    "        return state\n",
    "\n",
    "    def unselect_sentences(self, weights, state, to_remove):\n",
    "        \"\"\"Sentence ``un-selector'' (reverse operation of the\n",
    "          select_sentences method).\n",
    "\n",
    "        Args:\n",
    "            weights (dictionary): the sentence weights dictionary. This\n",
    "              dictionnary is updated during this method call (in-place).\n",
    "            state (State): the state of the tabu search from which to start\n",
    "              un-selecting sentences.\n",
    "            to_remove (iterable): set of sentences to unselect.\n",
    "\n",
    "        Returns:\n",
    "            state (State): the new state of the search. Also note that\n",
    "              weights is modified in-place.\n",
    "\n",
    "        \"\"\"\n",
    "        # remove the sentence indices from the solution subset\n",
    "        state.subset -= to_remove\n",
    "        for sentence_index in to_remove:\n",
    "            # update state\n",
    "            state.concepts.subtract(self.concept_sets[sentence_index])\n",
    "            state.length -= self.sentences[sentence_index].length\n",
    "            # update sentence weights with the reverse index\n",
    "            for concept in set(self.concept_sets[sentence_index]):\n",
    "                if not state.concepts[concept]:\n",
    "                    for sentence in self.c2s[concept]:\n",
    "                        weights[sentence] += self.weights[concept]\n",
    "            state.score -= weights[sentence_index]\n",
    "        return state\n",
    "\n",
    "    def solve_ilp_problem(self,\n",
    "                          summary_size=100,\n",
    "                          solver='glpk',\n",
    "                          excluded_solutions=[],\n",
    "                          unique=False):\n",
    "        \"\"\"Solve the ILP formulation of the concept-based model.\n",
    "\n",
    "        Args:\n",
    "            summary_size (int): the maximum size in words of the summary,\n",
    "              defaults to 100.\n",
    "            solver (str): the solver used, defaults to glpk.\n",
    "            excluded_solutions (list of list): a list of subsets of sentences\n",
    "              that are to be excluded, defaults to []\n",
    "            unique (bool): modify the model so that it produces only one optimal\n",
    "              solution, defaults to False\n",
    "\n",
    "        Returns:\n",
    "            (value, set) tuple (int, list): the value of the objective function\n",
    "              and the set of selected sentences as a tuple.\n",
    "\n",
    "        \"\"\"\n",
    "        # initialize container shortcuts\n",
    "        concepts = self.weights.keys()\n",
    "        w = self.weights\n",
    "        L = summary_size\n",
    "        C = len(concepts)\n",
    "        S = len(self.sentences)\n",
    "\n",
    "        if not self.word_frequencies:\n",
    "            self.compute_word_frequency()\n",
    "\n",
    "        tokens = self.word_frequencies.keys()\n",
    "        f = self.word_frequencies\n",
    "        T = len(tokens)\n",
    "\n",
    "        # HACK Sort keys\n",
    "        concepts = sorted(self.weights, key=self.weights.get, reverse=True)\n",
    "\n",
    "        # formulation of the ILP problem\n",
    "        prob = pulp.LpProblem(self.input_directory, pulp.LpMaximize)\n",
    "\n",
    "        # initialize the concepts binary variables\n",
    "        c = pulp.LpVariable.dicts(name='c',\n",
    "                                  indexs=range(C),\n",
    "                                  lowBound=0,\n",
    "                                  upBound=1,\n",
    "                                  cat='Integer')\n",
    "\n",
    "        # initialize the sentences binary variables\n",
    "        s = pulp.LpVariable.dicts(name='s',\n",
    "                                  indexs=range(S),\n",
    "                                  lowBound=0,\n",
    "                                  upBound=1,\n",
    "                                  cat='Integer')\n",
    "\n",
    "        # initialize the word binary variables\n",
    "        t = pulp.LpVariable.dicts(name='t',\n",
    "                                  indexs=range(T),\n",
    "                                  lowBound=0,\n",
    "                                  upBound=1,\n",
    "                                  cat='Integer')\n",
    "\n",
    "        # OBJECTIVE FUNCTION\n",
    "        prob += sum(w[concepts[i]] * c[i] for i in range(C))\n",
    "               \n",
    "        if unique:\n",
    "            prob += sum(w[concepts[i]] * c[i] for i in range(C)) + \\\n",
    "                    10e-6 * sum(f[tokens[k]] * t[k] for k in range(T))\n",
    "\n",
    "        # CONSTRAINT FOR SUMMARY SIZE\n",
    "        prob += sum(s[j] * self.sentences[j].length for j in range(S)) <= L\n",
    "\n",
    "        # INTEGRITY CONSTRAINTS\n",
    "        for i in range(C):\n",
    "            for j in range(S):\n",
    "                if concepts[i] in self.sentences[j].concepts:\n",
    "                    prob += s[j] <= c[i]\n",
    "\n",
    "        for i in range(C):\n",
    "            prob += sum(s[j] for j in range(S)\n",
    "                        if concepts[i] in self.sentences[j].concepts) >= c[i]\n",
    "\n",
    "        # WORD INTEGRITY CONSTRAINTS\n",
    "        if unique:\n",
    "            for k in range(T):\n",
    "                for j in self.w2s[tokens[k]]:\n",
    "                    prob += s[j] <= t[k]\n",
    "\n",
    "            for k in range(T):\n",
    "                prob += sum(s[j] for j in self.w2s[tokens[k]]) >= t[k]\n",
    "\n",
    "        # CONSTRAINTS FOR FINDING OPTIMAL SOLUTIONS\n",
    "        for sentence_set in excluded_solutions:\n",
    "            prob += sum([s[j] for j in sentence_set]) <= len(sentence_set)-1\n",
    "\n",
    "        # prob.writeLP('test.lp')\n",
    "\n",
    "        # solving the ilp problem\n",
    "        if solver == 'gurobi':\n",
    "            prob.solve(pulp.GUROBI(msg=0))\n",
    "        elif solver == 'glpk':\n",
    "            prob.solve(pulp.GLPK(msg=0))\n",
    "        elif solver == 'cplex':\n",
    "            prob.solve(pulp.CPLEX(msg=0))\n",
    "        else:\n",
    "            sys.exit('no solver specified')\n",
    "\n",
    "        # retreive the optimal subset of sentences\n",
    "        solution = set([j for j in range(S) if s[j].varValue == 1])\n",
    "\n",
    "        # returns the (objective function value, solution) tuple\n",
    "        return (pulp.value(prob.objective), solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dir_path = \"C:/Users/kennd/Downloads/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s = ConceptBasedILPSummarizer(dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s.read_documents(file_extension=\"txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s.extract_ngrams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
