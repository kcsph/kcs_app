{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get Soup\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import dateutil.parser\n",
    "from urllib.request import urlopen\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Get data from commentary url. Lowest level\n",
    "def get_commentary_data(url):\n",
    "    browser.open(url)\n",
    "    soup = BeautifulSoup(browser.response.text,'lxml')\n",
    "\n",
    "    ##Get author\n",
    "    asses = soup.findAll('a')\n",
    "    for a in asses:\n",
    "        try:\n",
    "            if 'author' in a['class']:\n",
    "                author.append(a.text)\n",
    "                break\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    ##Get title\n",
    "    title = soup.find('h1').text\n",
    "    titles.append(title)\n",
    "\n",
    "    ##Get article date\n",
    "    datepublished.append(soup.find('time').text.title())\n",
    "\n",
    "    ##Get text\n",
    "    mydivs = soup.findAll('div')\n",
    "    for div in mydivs:\n",
    "        try:\n",
    "            if div['itemprop'] == 'articleBody':\n",
    "                article_div = div\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    text = \"\"\n",
    "    for i in article_div.findAll('p'):\n",
    "        text += i.text\n",
    "\n",
    "    articles.append(text)\n",
    "\n",
    "def commentarylistscrape(topicurl):\n",
    "    browser.open(topicurl)\n",
    "    soup = BeautifulSoup(browser.response.text,'lxml')\n",
    "    commentaries = soup.findAll('h2')\n",
    "    for commentary in commentaries:\n",
    "        try:\n",
    "            if \"commentary\" in commentary.find('a')['href']:\n",
    "                commentarylist.append('https://www.project-syndicate.org'+commentary.find('a')['href'])\n",
    "                topicmarker.append(topicurl.replace('http://www.project-syndicate.org/',''))\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Get urls of all archive pages\n",
    "response = requests.get('https://www.project-syndicate.org/archive')\n",
    "soup = BeautifulSoup(response.text,'lxml')\n",
    "\n",
    "max_page = int(soup.find(text=re.compile('pages')).strip().split()[0])\n",
    "\n",
    "archive_pages_url = []\n",
    "for x in range(1,max_page+1):\n",
    "    archive_pages_url.append('https://www.project-syndicate.org/archive?page='+str(x))\n",
    "\n",
    "response = requests.get(archive_pages_url[0])\n",
    "soup = BeautifulSoup(response.text,'lxml')\n",
    "\n",
    "commentarylist=[]\n",
    "topicmarker = []\n",
    "\n",
    "for archive in archive_pages_url:\n",
    "    commentarylistscrape(archive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kennd\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "author=[]\n",
    "datepublished = []\n",
    "articles = []\n",
    "titles = []\n",
    "\n",
    "## Robobrowser\n",
    "from robobrowser import RoboBrowser\n",
    "\n",
    "browser = RoboBrowser()\n",
    "browser.open('http://project-syndicate.org/login')\n",
    "\n",
    "# Get the signup form\n",
    "signup_form = browser.get_form(class_='login user-input-form')\n",
    "# signup_form         # <RoboForm user[name]=, user[email]=, ...\n",
    "\n",
    "# # Inspect its values\n",
    "# signup_form['authenticity_token'].value     # 6d03597 ...\n",
    "\n",
    "# # Fill it out\n",
    "signup_form['email'].value = 'kenndanielso@gmail.com'\n",
    "signup_form['password'].value = '123qweQWE'\n",
    "\n",
    "# # Submit the form\n",
    "browser.submit_form(signup_form)\n",
    "\n",
    "## Extract Data\n",
    "for commentary in commentarylist:\n",
    "    get_commentary_data(commentary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ps_df.to_pickle('ps_df_2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ps_df = pd.read_pickle('ps_df_2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
