{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import textblob\n",
    "from textblob import TextBlob\n",
    "import dateutil\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from nltk.corpus import stopwords ##Note you'll need to download NLTK and corpuses\n",
    "from spacy.en import English ##Note you'll need to install Spacy and download its dependencies\n",
    "parser = English()\n",
    "import string\n",
    "import re\n",
    "import gensim\n",
    "\n",
    "import dateutil.parser\n",
    "import numpy as np\n",
    "import dateutil\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import networkx as nx\n",
    "from __future__ import print_function\n",
    "import array\n",
    "# coding=utf-8\n",
    "\n",
    "class Status(object):\n",
    "    \"\"\"\n",
    "    To handle several data in one struct.\n",
    "\n",
    "    Could be replaced by named tuple, but don't want to depend on python 2.6\n",
    "    \"\"\"\n",
    "    node2com = {}\n",
    "    total_weight = 0\n",
    "    internals = {}\n",
    "    degrees = {}\n",
    "    gdegrees = {}\n",
    "\n",
    "    def __init__(self):\n",
    "        self.node2com = dict([])\n",
    "        self.total_weight = 0\n",
    "        self.degrees = dict([])\n",
    "        self.gdegrees = dict([])\n",
    "        self.internals = dict([])\n",
    "        self.loops = dict([])\n",
    "\n",
    "    def __str__(self):\n",
    "        return (\"node2com : \" + str(self.node2com) + \" degrees : \"\n",
    "                + str(self.degrees) + \" internals : \" + str(self.internals)\n",
    "                + \" total_weight : \" + str(self.total_weight))\n",
    "\n",
    "    def copy(self):\n",
    "        \"\"\"Perform a deep copy of status\"\"\"\n",
    "        new_status = Status()\n",
    "        new_status.node2com = self.node2com.copy()\n",
    "        new_status.internals = self.internals.copy()\n",
    "        new_status.degrees = self.degrees.copy()\n",
    "        new_status.gdegrees = self.gdegrees.copy()\n",
    "        new_status.total_weight = self.total_weight\n",
    "\n",
    "    def init(self, graph, weight, part=None):\n",
    "        \"\"\"Initialize the status of a graph with every node in one community\"\"\"\n",
    "        count = 0\n",
    "        self.node2com = dict([])\n",
    "        self.total_weight = 0\n",
    "        self.degrees = dict([])\n",
    "        self.gdegrees = dict([])\n",
    "        self.internals = dict([])\n",
    "        self.total_weight = graph.size(weight=weight)\n",
    "        if part is None:\n",
    "            for node in graph.nodes():\n",
    "                self.node2com[node] = count\n",
    "                deg = float(graph.degree(node, weight=weight))\n",
    "                if deg < 0:\n",
    "                    error = \"Bad graph type ({})\".format(type(graph))\n",
    "                    raise ValueError(error)\n",
    "                self.degrees[count] = deg\n",
    "                self.gdegrees[node] = deg\n",
    "                edge_data = graph.get_edge_data(node, node, {weight: 0})\n",
    "                self.loops[node] = float(edge_data.get(weight, 1))\n",
    "                self.internals[count] = self.loops[node]\n",
    "                count += 1\n",
    "        else:\n",
    "            for node in graph.nodes():\n",
    "                com = part[node]\n",
    "                self.node2com[node] = com\n",
    "                deg = float(graph.degree(node, weight=weight))\n",
    "                self.degrees[com] = self.degrees.get(com, 0) + deg\n",
    "                self.gdegrees[node] = deg\n",
    "                inc = 0.\n",
    "                for neighbor, datas in graph[node].items():\n",
    "                    edge_weight = datas.get(weight, 1)\n",
    "                    if edge_weight <= 0:\n",
    "                        error = \"Bad graph type ({})\".format(type(graph))\n",
    "                        raise ValueError(error)\n",
    "                    if part[neighbor] == com:\n",
    "                        if neighbor == node:\n",
    "                            inc += float(edge_weight)\n",
    "                        else:\n",
    "                            inc += float(edge_weight) / 2.\n",
    "                self.internals[com] = self.internals.get(com, 0) + inc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "__author__ = \"\"\"Thomas Aynaud (thomas.aynaud@lip6.fr)\"\"\"\n",
    "\n",
    "__PASS_MAX = -1\n",
    "__MIN = 0.0000001\n",
    "\n",
    "\n",
    "def partition_at_level(dendrogram, level):\n",
    "  \n",
    "    partition = dendrogram[0].copy()\n",
    "    for index in range(1, level + 1):\n",
    "        for node, community in partition.items():\n",
    "            partition[node] = dendrogram[index][community]\n",
    "    return partition\n",
    "\n",
    "\n",
    "def modularity(partition, graph, weight='weight'):\n",
    " \n",
    "    if type(graph) != nx.Graph:\n",
    "        raise TypeError(\"Bad graph type, use only non directed graph\")\n",
    "\n",
    "    inc = dict([])\n",
    "    deg = dict([])\n",
    "    links = graph.size(weight=weight)\n",
    "    if links == 0:\n",
    "        raise ValueError(\"A graph without link has an undefined modularity\")\n",
    "\n",
    "    for node in graph:\n",
    "        com = partition[node]\n",
    "        deg[com] = deg.get(com, 0.) + graph.degree(node, weight=weight)\n",
    "        for neighbor, datas in graph[node].items():\n",
    "            edge_weight = datas.get(weight, 1)\n",
    "            if partition[neighbor] == com:\n",
    "                if neighbor == node:\n",
    "                    inc[com] = inc.get(com, 0.) + float(edge_weight)\n",
    "                else:\n",
    "                    inc[com] = inc.get(com, 0.) + float(edge_weight) / 2.\n",
    "\n",
    "    res = 0.\n",
    "    for com in set(partition.values()):\n",
    "        res += (inc.get(com, 0.) / links) - \\\n",
    "               (deg.get(com, 0.) / (2. * links)) ** 2\n",
    "    return res\n",
    "\n",
    "\n",
    "def best_partition(graph, partition=None, weight='weight', resolution=1.):\n",
    " \n",
    "    dendo = generate_dendrogram(graph, partition, weight, resolution)\n",
    "    return partition_at_level(dendo, len(dendo) - 1)\n",
    "\n",
    "\n",
    "def generate_dendrogram(graph, part_init=None, weight='weight', resolution=1.):\n",
    " \n",
    "    if type(graph) != nx.Graph:\n",
    "        raise TypeError(\"Bad graph type, use only non directed graph\")\n",
    "\n",
    "    # special case, when there is no link\n",
    "    # the best partition is everyone in its community\n",
    "    if graph.number_of_edges() == 0:\n",
    "        part = dict([])\n",
    "        for node in graph.nodes():\n",
    "            part[node] = node\n",
    "        return [part]\n",
    "\n",
    "    current_graph = graph.copy()\n",
    "    status = Status()\n",
    "    status.init(current_graph, weight, part_init)\n",
    "    status_list = list()\n",
    "    __one_level(current_graph, status, weight, resolution)\n",
    "    new_mod = __modularity(status)\n",
    "    partition = __renumber(status.node2com)\n",
    "    status_list.append(partition)\n",
    "    mod = new_mod\n",
    "    current_graph = induced_graph(partition, current_graph, weight)\n",
    "    status.init(current_graph, weight)\n",
    "\n",
    "    while True:\n",
    "        __one_level(current_graph, status, weight, resolution)\n",
    "        new_mod = __modularity(status)\n",
    "        if new_mod - mod < __MIN:\n",
    "            break\n",
    "        partition = __renumber(status.node2com)\n",
    "        status_list.append(partition)\n",
    "        mod = new_mod\n",
    "        current_graph = induced_graph(partition, current_graph, weight)\n",
    "        status.init(current_graph, weight)\n",
    "    return status_list[:]\n",
    "\n",
    "\n",
    "def induced_graph(partition, graph, weight=\"weight\"):\n",
    "\n",
    "    ret = nx.Graph()\n",
    "    ret.add_nodes_from(partition.values())\n",
    "\n",
    "    for node1, node2, datas in graph.edges_iter(data=True):\n",
    "        edge_weight = datas.get(weight, 1)\n",
    "        com1 = partition[node1]\n",
    "        com2 = partition[node2]\n",
    "        w_prec = ret.get_edge_data(com1, com2, {weight: 0}).get(weight, 1)\n",
    "        ret.add_edge(com1, com2, attr_dict={weight: w_prec + edge_weight})\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "def __renumber(dictionary):\n",
    "    \"\"\"Renumber the values of the dictionary from 0 to n\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    ret = dictionary.copy()\n",
    "    new_values = dict([])\n",
    "\n",
    "    for key in dictionary.keys():\n",
    "        value = dictionary[key]\n",
    "        new_value = new_values.get(value, -1)\n",
    "        if new_value == -1:\n",
    "            new_values[value] = count\n",
    "            new_value = count\n",
    "            count += 1\n",
    "        ret[key] = new_value\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "def __load_binary(data):\n",
    "    \"\"\"Load binary graph as used by the cpp implementation of this algorithm\n",
    "    \"\"\"\n",
    "    data = open(data, \"rb\")\n",
    "\n",
    "    reader = array.array(\"I\")\n",
    "    reader.fromfile(data, 1)\n",
    "    num_nodes = reader.pop()\n",
    "    reader = array.array(\"I\")\n",
    "    reader.fromfile(data, num_nodes)\n",
    "    cum_deg = reader.tolist()\n",
    "    num_links = reader.pop()\n",
    "    reader = array.array(\"I\")\n",
    "    reader.fromfile(data, num_links)\n",
    "    links = reader.tolist()\n",
    "    graph = nx.Graph()\n",
    "    graph.add_nodes_from(range(num_nodes))\n",
    "    prec_deg = 0\n",
    "\n",
    "    for index in range(num_nodes):\n",
    "        last_deg = cum_deg[index]\n",
    "        neighbors = links[prec_deg:last_deg]\n",
    "        graph.add_edges_from([(index, int(neigh)) for neigh in neighbors])\n",
    "        prec_deg = last_deg\n",
    "\n",
    "    return graph\n",
    "\n",
    "\n",
    "def __one_level(graph, status, weight_key, resolution):\n",
    "    \"\"\"Compute one level of communities\n",
    "    \"\"\"\n",
    "    modified = True\n",
    "    nb_pass_done = 0\n",
    "    cur_mod = __modularity(status)\n",
    "    new_mod = cur_mod\n",
    "\n",
    "    while modified and nb_pass_done != __PASS_MAX:\n",
    "        cur_mod = new_mod\n",
    "        modified = False\n",
    "        nb_pass_done += 1\n",
    "\n",
    "        for node in graph.nodes():\n",
    "            com_node = status.node2com[node]\n",
    "            degc_totw = status.gdegrees.get(node, 0.) / (status.total_weight * 2.)  # NOQA\n",
    "            neigh_communities = __neighcom(node, graph, status, weight_key)\n",
    "            __remove(node, com_node,\n",
    "                     neigh_communities.get(com_node, 0.), status)\n",
    "            best_com = com_node\n",
    "            best_increase = 0\n",
    "            for com, dnc in neigh_communities.items():\n",
    "                incr = resolution * dnc - \\\n",
    "                       status.degrees.get(com, 0.) * degc_totw\n",
    "                if incr > best_increase:\n",
    "                    best_increase = incr\n",
    "                    best_com = com\n",
    "            __insert(node, best_com,\n",
    "                     neigh_communities.get(best_com, 0.), status)\n",
    "            if best_com != com_node:\n",
    "                modified = True\n",
    "        new_mod = __modularity(status)\n",
    "        if new_mod - cur_mod < __MIN:\n",
    "            break\n",
    "\n",
    "\n",
    "def __neighcom(node, graph, status, weight_key):\n",
    "\n",
    "    weights = {}\n",
    "    for neighbor, datas in graph[node].items():\n",
    "        if neighbor != node:\n",
    "            edge_weight = datas.get(weight_key, 1)\n",
    "            neighborcom = status.node2com[neighbor]\n",
    "            weights[neighborcom] = weights.get(neighborcom, 0) + edge_weight\n",
    "\n",
    "    return weights\n",
    "\n",
    "\n",
    "def __remove(node, com, weight, status):\n",
    "    \"\"\" Remove node from community com and modify status\"\"\"\n",
    "    status.degrees[com] = (status.degrees.get(com, 0.)\n",
    "                           - status.gdegrees.get(node, 0.))\n",
    "    status.internals[com] = float(status.internals.get(com, 0.) -\n",
    "                                  weight - status.loops.get(node, 0.))\n",
    "    status.node2com[node] = -1\n",
    "\n",
    "\n",
    "def __insert(node, com, weight, status):\n",
    "    \"\"\" Insert node into community and modify status\"\"\"\n",
    "    status.node2com[node] = com\n",
    "    status.degrees[com] = (status.degrees.get(com, 0.) +\n",
    "                           status.gdegrees.get(node, 0.))\n",
    "    status.internals[com] = float(status.internals.get(com, 0.) +\n",
    "                                  weight + status.loops.get(node, 0.))\n",
    "\n",
    "\n",
    "def __modularity(status):\n",
    "    \"\"\"\n",
    "    Fast compute the modularity of the partition of the graph using\n",
    "    status precomputed\n",
    "    \"\"\"\n",
    "    links = float(status.total_weight)\n",
    "    result = 0.\n",
    "    for community in set(status.node2com.values()):\n",
    "        in_degree = status.internals.get(community, 0.)\n",
    "        degree = status.degrees.get(community, 0.)\n",
    "        if links > 0:\n",
    "            result += in_degree / links - ((degree / (2. * links)) ** 2)\n",
    "    return result\n",
    "\n",
    "\n",
    "# A custom function to clean the text before sending it into the vectorizer\n",
    "def cleanText(text):\n",
    "    # get rid of newlines\n",
    "    text = text.strip().replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "    \n",
    "    # replace twitter @mentions\n",
    "    mentionFinder = re.compile(r\"@[a-z0-9_]{1,15}\", re.IGNORECASE)\n",
    "    text = mentionFinder.sub(\"@MENTION\", text)\n",
    "    text = re.sub('[^a-zA-Z ]','',text)\n",
    "    # replace HTML symbols\n",
    "    text = text.replace(\"&amp;\", \"and\").replace(\"&gt;\", \">\").replace(\"&lt;\", \"<\")\n",
    "    \n",
    "    # lowercase\n",
    "    text = text.lower()\n",
    "#     text = str(TextBlob(text).correct())\n",
    "    return text\n",
    "\n",
    "# A custom function to tokenize the text using spaCy\n",
    "# and convert to lemmas\n",
    "def tokenizeText(sample):\n",
    "    # get the tokens using spaCy\n",
    "    tokens = parser(cleanText(sample))\n",
    "\n",
    "    # lemmatize\n",
    "    lemmas = []\n",
    "    for tok in tokens:\n",
    "        lemmas.append(tok.lemma_.lower().strip() if tok.lemma_ != \"-PRON-\" else tok.lower_)\n",
    "    tokens = lemmas\n",
    "\n",
    "    # stoplist the tokens\n",
    "    tokens = [tok for tok in tokens if tok not in STOPLIST]\n",
    "\n",
    "    # stoplist symbols\n",
    "    tokens = [tok for tok in tokens if tok not in SYMBOLS]\n",
    "\n",
    "    # remove large strings of whitespace\n",
    "    while \"\" in tokens:\n",
    "        tokens.remove(\"\")\n",
    "    while \" \" in tokens:\n",
    "        tokens.remove(\" \")\n",
    "    while \"\\n\" in tokens:\n",
    "        tokens.remove(\"\\n\")\n",
    "    while \"\\n\\n\" in tokens:\n",
    "        tokens.remove(\"\\n\\n\")\n",
    "\n",
    "    return tokens\n",
    "\n",
    "# A custom stoplist\n",
    "STOPLIST = set(stopwords.words('english') + [\"n't\", \"'s\", \"'m\", \"ca\"] + list(ENGLISH_STOP_WORDS))\n",
    "# List of symbols we don't care about\n",
    "SYMBOLS = \" \".join(string.punctuation).split(\" \") + [\"-----\", \"---\", \"...\", \"“\", \"”\", \"'ve\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch, RequestsHttpConnection\n",
    "from requests_aws4auth import AWS4Auth\n",
    "import urllib3\n",
    "import boto3\n",
    "\n",
    "\n",
    "host = 'search-trial-cc4abhfofwbjogy5nla5sndexq.us-west-2.es.amazonaws.com'\n",
    "aws_auth = AWS4Auth('AKIAJQ5JJZM5HDDOMTNQ', 'uVqOuxD+e/iaLkfusHi4TgO1wqrSdUsz2I+VLoAS', 'us-west-2', 'es')\n",
    "\n",
    "es = Elasticsearch(\n",
    "    hosts=[{'host': host, 'port': 443}],\n",
    "#     http_auth=aws_auth,\n",
    "    use_ssl=True,\n",
    "    verify_certs=True,\n",
    "    connection_class=RequestsHttpConnection\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "matches = es.search(index='weiboscope', q='GDP finance', size=500)\n",
    "hits = matches['hits']['hits']\n",
    "query_df = pd.DataFrame(list(pd.DataFrame(hits)['_source']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Convert to date time\n",
    "def conv_date(x):\n",
    "    return dateutil.parser.parse(x)\n",
    "\n",
    "query_df.date = query_df.date.apply(conv_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Add sentiment analysis\n",
    "sentiment_score = []\n",
    "for row in range(query_df.shape[0]):\n",
    "    sentiment_score.append(TextBlob(query_df.loc[row,'article']).sentiment.polarity)\n",
    "sentiment_score = pd.Series(sentiment_score)\n",
    "\n",
    "##Normalize sentiment score\n",
    "sentiment_score = ((sentiment_score + abs(sentiment_score.min()))/(sentiment_score.max()+abs(sentiment_score.min())))\n",
    "\n",
    "query_df['sent_score'] = sentiment_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Add length of article\n",
    "length = []\n",
    "for row in range(query_df.shape[0]):\n",
    "    length.append(len(query_df.loc[row,'article'].split()))\n",
    "query_df['length'] = pd.Series(length)\n",
    "\n",
    "## Limit to articles with over 500 words only\n",
    "query_df = query_df[query_df['length']>=500].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Add summary to each article\n",
    "summaries = []\n",
    "for row in range(query_df.shape[0]):\n",
    "    summaries.append(gensim.summarization.summarize(query_df.loc[row,'article'],ratio=0.1))\n",
    "query_df['summary'] = pd.Series(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Countvectorizer\n",
    "tfidfvectorizer = TfidfVectorizer(tokenizer=tokenizeText,strip_accents='unicode',ngram_range=(1,4),min_df=0.01,max_df=0.99,max_features=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf_vector = tfidfvectorizer.fit_transform(query_df.loc[:,'article'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfidf_df = pd.DataFrame(tfidf_vector.A,columns=tfidfvectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# query_df.columns = ['title_ps','author_ps','date_ps','article_ps','source_ps','sent_score','article_length','summary_ps']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# processed_df = pd.concat((query_df,tfidf_df),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# processed_df.to_pickle('processed_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# processed_df = pd.read_pickle('C:/Users/kennd/Documents/Github/kcs_app/app/processed_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# processed_df1 = processed_df.iloc[:,0:1500]\n",
    "# processed_df2 = processed_df.iloc[:,1501:3000]\n",
    "# processed_df3 = processed_df.iloc[:,0:3001:4500]\n",
    "# processed_df4 = processed_df.iloc[:,0:4501:6000]\n",
    "# processed_df5 = processed_df.iloc[:,0:6001:7500]\n",
    "# processed_df6 = processed_df.iloc[:,0:7501:9000]\n",
    "# processed_df7 = processed_df.iloc[:,0:9001:]\n",
    "\n",
    "# from sqlalchemy import create_engine\n",
    "# import pandas as pd\n",
    "# import json\n",
    "\n",
    "# cred = json.load(open('dbcred.json'))\n",
    "\n",
    "# engine = create_engine('postgresql://{user}:{password}@{server}/{db}'.format(\n",
    "#         user=cred[\"user\"],\n",
    "#         password=cred[\"password\"],\n",
    "#         server=cred[\"server\"],\n",
    "#         db=cred['db']))\n",
    "\n",
    "# processed_df1.to_sql('processed_df1',engine,if_exists='replace')\n",
    "# processed_df2.to_sql('processed_df2',engine,if_exists='replace')\n",
    "# processed_df3.to_sql('processed_df3',engine,if_exists='replace')\n",
    "# processed_df4.to_sql('processed_df4',engine,if_exists='replace')\n",
    "# processed_df5.to_sql('processed_df5',engine,if_exists='replace')\n",
    "# processed_df6.to_sql('processed_df6',engine,if_exists='replace')\n",
    "# processed_df7.to_sql('processed_df7',engine,if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Network Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Calculate similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Network generation\n",
    "## First calculate similarity between articles\n",
    "\n",
    "norms = np.sqrt(np.sum(tfidf_vector.A * tfidf_vector.A, axis=1, keepdims=True))  # multiplication between arrays is element-wise\n",
    "\n",
    "query_tfidf_normed = tfidf_vector / norms\n",
    "\n",
    "weights = np.dot(query_tfidf_normed, query_tfidf_normed.T).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Generate graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = nx.Graph()\n",
    "graph.add_edges_from(\n",
    "    (i, j, {'weight': weights[i][j]})\n",
    "    for i in range(tfidf_vector.shape[0]) for j in range(i + 1, tfidf_vector.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Cluster graphs into groupings based on maximum modularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "partition = best_partition(graph,resolution=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query_df['group_cluster'] = pd.Series(list(partition.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query_df.columns = ['Text','Author','Date','Source','Title','Sentiment','Length','Summary','Cluster']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create top words for each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_df = pd.concat((query_df,pd.DataFrame(tfidf_vector.A,columns=tfidfvectorizer.vocabulary_)),axis=1).groupby('Cluster').sum()\n",
    "\n",
    "del new_df['Sentiment']\n",
    "del new_df['Length']\n",
    "\n",
    "clusters = list(query_df['Cluster'].unique())\n",
    "\n",
    "cluster = []\n",
    "words = []\n",
    "for x in clusters:\n",
    "    cluster.append(x)\n",
    "    words.append(', '.join(list(new_df.loc[x,:].sort_values(ascending=False)[0:5].index)))\n",
    "\n",
    "cluster = pd.Series(cluster)\n",
    "words = pd.Series(words)\n",
    "\n",
    "cluster_df = pd.concat((cluster,words),axis=1)\n",
    "cluster_df.columns = ['Cluster','Top Words']\n",
    "\n",
    "query_df = query_df.merge(cluster_df,on='Cluster',how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload to SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "cred = json.load(open('dbcred.json'))\n",
    "\n",
    "engine = create_engine('postgresql://{user}:{password}@{server}/{db}'.format(\n",
    "        user=cred[\"user\"],\n",
    "        password=cred[\"password\"],\n",
    "        server=cred[\"server\"],\n",
    "        db=cred['db']))\n",
    "\n",
    "query_df.to_sql('queried_df',engine,if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
